import datetime
import os
import time
import shutil
import hydra
from omegaconf import DictConfig
from pytorch_lightning import seed_everything
from tqdm import tqdm
from hydra.utils import instantiate
from src.tool.utils import calculate_model_params


class LogFileHandler:
    def __init__(self, file_path):
        self.file_path = file_path
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

    def log(self, metrics):
        """å°†æŒ‡æ ‡é™„åŠ åˆ°æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(self.file_path, "a") as f:
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                log_line = f"{timestamp} - "
                log_line += ", ".join([f"{k}: {v}" for k, v in metrics.items()])
                f.write(log_line + "\n")
        except Exception as e:
            print(f"Error writing to log file: {e}")

@hydra.main(version_base=None, config_path="configs", config_name="gene_expansion")
def main(cfg: DictConfig):
    import torch
    print(f"æ”¯æŒçš„æž¶æž„: {torch.cuda.get_arch_list()}")
    print(f"è®¾å¤‡æž¶æž„: {torch.cuda.get_device_capability(0)}")
    print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")

    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['HF_MIRROR'] = 'https://hf-mirror.com'
    os.environ["HYDRA_FULL_ERROR"] = str(1)

    # åˆå§‹åŒ–Fabric (ç”¨äºŽåˆ†å¸ƒå¼è®­ç»ƒ)
    fabric = instantiate(cfg.machine.fabric)
    fabric.launch()

    # è®¾ç½®éšæœºç§å­
    seed_everything(cfg.train.seed)

    # åˆ›å»ºæ—¥å¿—å¤„ç†å™¨ (ä»…åœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–)
    log_file_path = os.path.join(datetime.datetime.now().strftime("%m-%d-%H-%M-%S"), "train_detail.log")
    file_logger = None
    if fabric.is_global_zero:
        file_logger = LogFileHandler(log_file_path)
        fabric.print(f"Logging to {log_file_path}")

    # åˆ›å»ºæ•°æ®æ¨¡å—
    dataset = instantiate(cfg.data, _recursive_=False)

    train_dataloader = fabric.setup_dataloaders(dataset.train_dataloader())
    if cfg.is_val:
        val_dataloader = fabric.setup_dataloaders(dataset.val_dataloader())

    # åˆ›å»ºæ¨¡åž‹
    model = instantiate(cfg.model)
    calculate_model_params(model)

    optimizer = instantiate(
        cfg.optimizer, params=model.parameters(), _partial_=False
    )
    model, optimizer = fabric.setup(model, optimizer)

    scheduler = instantiate(cfg.scheduler)

    # =============== æ–­ç‚¹ç»­è®­é€»è¾‘ ===============
    start_epoch = 0
    # å¦‚æžœéœ€è¦æ¢å¤è®­ç»ƒ
    if cfg.train.resume:
        resume_epoch = cfg.train.resume_epoch
        checkpoint_path = f"epoch_{resume_epoch}.ckpt"
        fabric.print(f"â© æ¢å¤è®­ç»ƒ: åŠ è½½ {checkpoint_path} (epoch {resume_epoch})")

        # åˆ›å»ºå®Œæ•´çŠ¶æ€å­—å…¸å¹¶åŠ è½½
        state = {
            "model": model,
            "optimizer": optimizer,
            "scheduler": scheduler,
        }

        try:
            # åŠ è½½æ£€æŸ¥ç‚¹
            fabric.load(checkpoint_path, state)
            start_epoch = resume_epoch + 1
            fabric.print(f"âœ… æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå°†ä»Ž epoch {start_epoch} ç»§ç»­è®­ç»ƒ")

        except FileNotFoundError:
            fabric.print(f"âš ï¸ æ£€æŸ¥ç‚¹ {checkpoint_path} æœªæ‰¾åˆ°ï¼Œä»Žå¤´å¼€å§‹è®­ç»ƒ")
        except Exception as e:
            fabric.print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {str(e)}")
            fabric.print("âš ï¸ å›žé€€åˆ°ä»Žå¤´å¼€å§‹è®­ç»ƒ")
    # =========================================

    if cfg.is_val:
        fabric.print("Start Validation!")
        val(model, val_dataloader, fabric)

    fabric.print(f"ðŸš€ å¼€å§‹è®­ç»ƒ (ä»Ž epoch {start_epoch} åˆ° {cfg.train.max_epochs - 1})")
    start_time = time.time()

    columns = shutil.get_terminal_size().columns
    fabric.print("-" * columns)
    fabric.print(cfg)

    for epoch in range(start_epoch, cfg.train.max_epochs):
        scheduler(optimizer, epoch)

        fabric.print("-" * columns)
        fabric.print(f"Epoch {epoch + 1}/{cfg.train.max_epochs}".center(columns))

        train(model, train_dataloader, optimizer, fabric, epoch, cfg, file_logger)

        state = {
            "epoch": epoch,
            "model": model,
            "optimizer": optimizer,
            "scheduler": scheduler,
        }

        fabric.save(f"epoch_{epoch}.ckpt", state)

        fabric.barrier()

        if cfg.is_val:
            fabric.print("Start Validation!")
            val(model, val_dataloader, fabric)

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    fabric.print(f"Training time {total_time_str}")

    fabric.logger.finalize("success")
    fabric.print(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))


def train(model, train_loader, optimizer, fabric, epoch, cfg, file_logger=None):
    model.train()

    if fabric.is_global_zero:
        pbar = tqdm(
            total=len(train_loader),
            desc=f"Epoch {epoch}",
            dynamic_ncols=True,
            position=0
        )
    else:
        pbar = None

    for batch_idx, batch in enumerate(train_loader):

        optimizer.zero_grad()
        loss = model.training_step(batch, batch_idx, fabric)
        fabric.backward(loss)
        optimizer.step()

        if pbar is not None:
            pbar.set_postfix({
                "loss": f"{loss.item():.10f}",
                "lr": f"{optimizer.param_groups[0]['lr']:.10f}"
            })
            pbar.update(1)

        if batch_idx % cfg.log.log_interval == 0:
            log_metrics = {
                "loss": loss.item(),
                "lr": optimizer.param_groups[0]["lr"],
                "epoch": epoch,
                "batch": batch_idx
            }

            # Fabricçš„æ ‡å‡†æ—¥å¿—
            fabric.log_dict(log_metrics)

            # è‡ªå®šä¹‰æ–‡ä»¶æ—¥å¿— (ä»…åœ¨ä¸»è¿›ç¨‹ä¸­)
            if file_logger is not None and fabric.is_global_zero:
                file_logger.log(log_metrics)

    if pbar is not None:
        pbar.close()


def val(model, val_loader, fabric):
    model.eval()
    total_metrics = {}
    count = 0
    if fabric.is_global_zero:
        pbar = tqdm(
            total=len(val_loader),
            desc=f"Validation",
            dynamic_ncols=True,
            position=0
        )
    else:
        pbar = None

    for batch_idx, batch in enumerate(val_loader):
        if batch_idx >= 1:
            break

        metrics = model.validation_step(batch, batch_idx, fabric, stage="val", solver="repaint_ddim")

        # ç´¯ç§¯æŒ‡æ ‡
        for k, v in metrics.items():
            if k not in total_metrics:
                total_metrics[k] = 0.0
            total_metrics[k] += v

        count += 1
        if pbar is not None:
            pbar.set_postfix({k: f"{v:.6f}" for k, v in metrics.items()})
            pbar.update(1)

    if pbar is not None:
        pbar.close()

if __name__ == "__main__":
    main()